{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0d070fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/700050058/.conda/envs/fastvideo/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-11-10 22:12:02.067271: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-10 22:12:02.384318: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-10 22:12:08.203224: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedback_text    2\n",
      "sentiment        0\n",
      "topic            0\n",
      "urgency          0\n",
      "action           0\n",
      "source           0\n",
      "dtype: int64\n",
      "    feedback_text sentiment    topic urgency action source\n",
      "49            NaN   neutral  overall     low   none   real\n",
      "139           NaN   neutral  overall     low   none   real\n",
      "['feedback_text', 'sentiment', 'topic', 'urgency', 'action', 'source']\n",
      "                                       feedback_text sentiment  \\\n",
      "0  We need to streamline the AI demos for all AI ...  negative   \n",
      "1                                 Room size is small  negative   \n",
      "2                 Noise outside the room in the hall  negative   \n",
      "3              The lunch was so cold and food is dry  negative   \n",
      "4                         No variety of food options  negative   \n",
      "5                      Coffee machine is not working  negative   \n",
      "6                         The content is so outdated  negative   \n",
      "7                                          Excellent  positive   \n",
      "8              The instructor is really professional  positive   \n",
      "9                Wasted money, I didn't pay for this  negative   \n",
      "\n",
      "             topic urgency                 action source  \n",
      "0  equipment/setup    high  modify course content   real  \n",
      "1            venue    high          contact venue   real  \n",
      "2            venue    high          contact venue   real  \n",
      "3         catering    high          contact venue   real  \n",
      "4         catering  medium          contact venue   real  \n",
      "5            venue    high          contact venue   real  \n",
      "6          content    high  modify course content   real  \n",
      "7          overall     low                   none   real  \n",
      "8       instructor     low                   none   real  \n",
      "9          overall    high               escalate   real  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 30\n",
    "LR = 2e-5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    import os\n",
    "    os.environ[\"TF_ENABLE_ONEDNN_OPTS\"]= \"0\"\n",
    "except ImportError:\n",
    "    print(\"Please check the environment.\")\n",
    "def check_input(df):\n",
    "    print(df.isna().sum())\n",
    "    print(df[df['feedback_text'].isna()])\n",
    "    print(df.columns.tolist())\n",
    "    print(df.head(10))\n",
    "\n",
    "check_input(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d5a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data class\n",
    "class FeedbackDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, rule_based_func=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.rule_based_func = rule_based_func or (lambda x: torch.zeros(20))  # ÈªòËÆ§ËßÑÂàôÁâπÂæÅÂêëÈáèÈïøÂ∫¶20\n",
    "\n",
    "        # ÁºñÁ†ÅÊ†áÁ≠æ\n",
    "        self.sentiment_encoder = LabelEncoder()\n",
    "        self.sentiment = torch.tensor(self.sentiment_encoder.fit_transform(self.data['sentiment']))\n",
    "        \n",
    "        self.urgency_encoder = LabelEncoder()\n",
    "        self.urgency = torch.tensor(self.urgency_encoder.fit_transform(self.data['urgency']))\n",
    "\n",
    "        self.topic_encoder = LabelEncoder()\n",
    "        self.topics = torch.tensor(\n",
    "            self.topic_encoder.fit_transform(self.data['topic']))\n",
    "\n",
    "        self.action_encoder = LabelEncoder()\n",
    "        self.action = torch.tensor(self.action_encoder.fit_transform(self.data['action']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.loc[idx, 'feedback_text']\n",
    "        rule_vec = self.rule_based_func(text)  # rule-based binary vector\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            text, truncation=True, padding='max_length', max_length=MAX_LEN, return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in inputs.items()}\n",
    "        item['rule_vec'] = rule_vec\n",
    "        item['sentiment'] = self.sentiment[idx]\n",
    "        item['urgency'] = self.urgency[idx]\n",
    "        item['topic'] = self.topics[idx]\n",
    "        item['action'] = self.action[idx]\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35c2e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule for bert class\n",
    "def rule_based_features(text):\n",
    "    sentiment_words = [\"excellent\", \"terrible\", \"good\", \"bad\", \"love\", \"hate\"]\n",
    "    urgency_words = [\"immediately\", \"urgent\", \"asap\", \"delay\"]\n",
    "    topic_words = [\"trainer\", \"venue\", \"content\", \"equipment\"]\n",
    "    all_words = sentiment_words + urgency_words + topic_words\n",
    "\n",
    "    vector = torch.zeros(len(all_words))\n",
    "    # print(text, type(text))\n",
    "    words = text.lower().split()\n",
    "    for i, w in enumerate(all_words):\n",
    "        if w in words:\n",
    "            vector[i] = 1.0\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8072b8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskBERT(nn.Module):\n",
    "    def __init__(self, model_name, rule_dim=20):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.rule_dim = rule_dim\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "\n",
    "        # ÂÜªÁªìÈô§ÊúÄÂêé‰∏§Â±ÇÂ§ñÁöÑTransformerÂ±Ç\n",
    "        for name, param in self.bert.named_parameters():\n",
    "            if \"encoder.layer.10\" not in name and \"encoder.layer.11\" not in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # ÂàÜÁ±ªÂ§¥\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.sentiment_classifier = nn.Linear(hidden_size + rule_dim, 3)\n",
    "        self.urgency_classifier = nn.Linear(hidden_size + rule_dim, 3)\n",
    "        self.topic_classifier = nn.Linear(hidden_size + rule_dim, 7)  # Â§öÊ†áÁ≠æ\n",
    "        self.action_classifier = nn.Linear(hidden_size + rule_dim, 6)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, rule_vec):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = torch.cat([cls_output, rule_vec], dim=1)\n",
    "        x = self.dropout(x)\n",
    "        return {\n",
    "            \"sentiment\": self.sentiment_classifier(x),\n",
    "            \"urgency\": self.urgency_classifier(x),\n",
    "            \"topic\": self.topic_classifier(x),\n",
    "            \"action\": self.action_classifier(x)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70b4123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do training curve\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    urg_preds, urg_labels = [], []\n",
    "    sent_preds, sent_labels = [], []\n",
    "    f1_scores = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            rule_vec = batch['rule_vec'].to(DEVICE)\n",
    "            sentiment = batch['sentiment'].to(DEVICE)\n",
    "            urgency = batch['urgency'].to(DEVICE)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, rule_vec)\n",
    "            s_pred = torch.argmax(outputs['sentiment'], dim=1)\n",
    "            u_pred = torch.argmax(outputs['urgency'], dim=1)\n",
    "\n",
    "            sent_preds.extend(s_pred.cpu().numpy())\n",
    "            sent_labels.extend(sentiment.cpu().numpy())\n",
    "            urg_preds.extend(u_pred.cpu().numpy())\n",
    "            urg_labels.extend(urgency.cpu().numpy())\n",
    "\n",
    "    f1_scores['sentiment'] = f1_score(sent_labels, sent_preds, average='macro')\n",
    "    f1_scores['urgency'] = f1_score(urg_labels, urg_preds, average='macro')\n",
    "    acc_sent = accuracy_score(sent_labels, sent_preds)\n",
    "    acc_urg = accuracy_score(urg_labels, urg_preds)\n",
    "\n",
    "    print(f\"üìä Validation | Sentiment: F1={f1_scores['sentiment']:.3f}, Acc={acc_sent:.3f} | \"\n",
    "          f\"Urgency: F1={f1_scores['urgency']:.3f}, Acc={acc_urg:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"f1_sentiment\": f1_scores['sentiment'],\n",
    "        \"f1_urgency\": f1_scores['urgency'],\n",
    "        \"acc_sentiment\": acc_sent,\n",
    "        \"acc_urgency\": acc_urg\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9521e20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running code\n",
    "def train_model(model, dataloader, optimizer, scheduler, loss_fn, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            rule_vec = batch['rule_vec'].to(DEVICE)\n",
    "            sentiment = batch['sentiment'].to(DEVICE)\n",
    "            urgency = batch['urgency'].to(DEVICE)\n",
    "            topic = batch['topic'].to(DEVICE)\n",
    "            action = batch['action'].to(DEVICE)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, rule_vec)\n",
    "            loss = (\n",
    "                0.2 * loss_fn['ce'](outputs['sentiment'], sentiment)\n",
    "                + 0.3 * loss_fn['ce'](outputs['urgency'], urgency)\n",
    "                + 0.4 * loss_fn['ce'](outputs['topic'], topic)\n",
    "                + loss_fn['ce'](outputs['action'], action)\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} average loss: {total_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9777a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train - val loop\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, loss_fn, num_epochs):\n",
    "    train_losses, val_f1s = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            rule_vec = batch['rule_vec'].to(DEVICE)\n",
    "            sentiment = batch['sentiment'].to(DEVICE)\n",
    "            urgency = batch['urgency'].to(DEVICE)\n",
    "            topic = batch['topic'].to(DEVICE)\n",
    "            action = batch['action'].to(DEVICE)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, rule_vec)\n",
    "            loss = (\n",
    "                0.25 * loss_fn['ce'](outputs['sentiment'], sentiment)\n",
    "                + 0.25 * loss_fn['ce'](outputs['urgency'], urgency)\n",
    "                + 0.35 * loss_fn['ce'](outputs['topic'], topic)\n",
    "                + 0.15 * loss_fn['ce'](outputs['action'], action)\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        print(f\"‚úÖ Epoch {epoch+1} | Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # È™åËØÅÈò∂ÊÆµ\n",
    "        val_metrics = evaluate(model, val_loader)\n",
    "        val_f1s.append((val_metrics['f1_sentiment'] + val_metrics['f1_urgency']) / 2)\n",
    "\n",
    "    return train_losses, val_f1s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64969b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 21/39 [00:27<00:23,  1.29s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=\u001b[32m0\u001b[39m, num_training_steps=total_steps)\n\u001b[32m     21\u001b[39m loss_fn = {\u001b[33m\"\u001b[39m\u001b[33mce\u001b[39m\u001b[33m\"\u001b[39m: nn.CrossEntropyLoss(), \u001b[33m\"\u001b[39m\u001b[33mbce\u001b[39m\u001b[33m\"\u001b[39m: nn.BCEWithLogitsLoss()}\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m train_losses, val_f1s = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# ÁªòÂà∂ËÆ≠ÁªÉÊçüÂ§±‰∏éÈ™åËØÅF1\u001b[39;00m\n\u001b[32m     25\u001b[39m plt.figure(figsize=(\u001b[32m8\u001b[39m,\u001b[32m4\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, scheduler, loss_fn, num_epochs)\u001b[39m\n\u001b[32m      6\u001b[39m model.train()\n\u001b[32m      7\u001b[39m total_loss = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/fastvideo/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/fastvideo/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/fastvideo/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/fastvideo/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mFeedbackDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m     26\u001b[39m     text = \u001b[38;5;28mself\u001b[39m.data.loc[idx, \u001b[33m'\u001b[39m\u001b[33mfeedback_text\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     rule_vec = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrule_based_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# rule-based binary vector\u001b[39;00m\n\u001b[32m     29\u001b[39m     inputs = \u001b[38;5;28mself\u001b[39m.tokenizer(\n\u001b[32m     30\u001b[39m         text, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, padding=\u001b[33m'\u001b[39m\u001b[33mmax_length\u001b[39m\u001b[33m'\u001b[39m, max_length=MAX_LEN, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m     )\n\u001b[32m     32\u001b[39m     item = {key: val.squeeze(\u001b[32m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mrule_based_features\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      8\u001b[39m vector = torch.zeros(\u001b[38;5;28mlen\u001b[39m(all_words))\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# print(text, type(text))\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m words = \u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m().split()\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_words):\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words:\n",
      "\u001b[31mAttributeError\u001b[39m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "val_df.to_csv(\"val.csv\", index=False)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "train_dataset = FeedbackDataset(\"train.csv\", tokenizer, rule_based_features)\n",
    "val_dataset = FeedbackDataset(\"val.csv\", tokenizer, rule_based_features)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "model = MultiTaskBERT(MODEL_NAME, rule_dim=14).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "loss_fn = {\"ce\": nn.CrossEntropyLoss(), \"bce\": nn.BCEWithLogitsLoss()}\n",
    "train_losses, val_f1s = train_model(model, train_loader, val_loader, optimizer, scheduler, loss_fn, EPOCHS)\n",
    "\n",
    "# ÁªòÂà∂ËÆ≠ÁªÉÊçüÂ§±‰∏éÈ™åËØÅF1\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(train_losses, label='Training Loss', marker='o')\n",
    "plt.plot(val_f1s, label='Validation F1 (avg)', marker='x')\n",
    "plt.title(\"Training & Validation Progress\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastvideo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
